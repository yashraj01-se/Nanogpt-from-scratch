{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rBJEJedhEd1"
      },
      "source": [
        "#Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rEDVk-zhENE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBCTqpXXbPEU"
      },
      "source": [
        "#Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r41uLxhs3zqE",
        "outputId": "a38d2049-edd0-4161-881a-829b213f93c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40RgrDN9bM0-",
        "outputId": "24c98ed6-c323-4cc6-b9c2-f5c9db8848aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "with open(\"tiny_shakespeare.txt\",encoding='utf-8') as f:\n",
        "  text=f.read()\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXDmhx6hb4oA",
        "outputId": "20472771-5e27-4087-fd48-1c0c012c38b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text file: 1115394\n"
          ]
        }
      ],
      "source": [
        "#Observation about the text:\n",
        "text_lenght=len(text)\n",
        "print(f\"Length of text file: {text_lenght}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pba5t2omccQ2",
        "outputId": "b9b4a43e-bb17-44ef-e58a-d6cdb87fcf30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size: 65\n",
            "Characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "#Observation about the text:\n",
        "characters=sorted(list(set(text)))\n",
        "vocab_size=len(characters)\n",
        "print(f\"Vocab Size: {vocab_size}\")\n",
        "print(f\"Characters: {''.join(characters)}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5gE1Qwhe7oq",
        "outputId": "f80c910b-9898-4010-a402-1827b16952e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[37, 39, 57, 46, 56, 39, 48, 1, 31, 46, 39, 56, 51, 39]\n",
            "Yashraj Sharma\n"
          ]
        }
      ],
      "source": [
        "#Tokenizing Strategic:\n",
        "#->Create A lookup Table\n",
        "#We could also used the Google's sentencepiece but it is a sub-word based tokenizer but here we are tokenizing each character of the word...\n",
        "#GPT uses tiktoken... Very good in tokenizing long sequences...\n",
        "#Mapping Characters to integer:-(Embedding)(Character level)\n",
        "#At this stage, this is not an embedding yet—this is indexing.\n",
        "Chrac_2_idx={ch:i for i,ch in enumerate(characters)}\n",
        "idx_2_Chrac={i:ch for i,ch in enumerate(characters)}\n",
        "encode=lambda s:[Chrac_2_idx[i] for i in s] #encoding each Character into interger\n",
        "decode=lambda l:''.join([idx_2_Chrac[i] for i in l]) #Decoding the interger to it srespective Character\n",
        "\n",
        "print(encode(\"Yashraj Sharma\"))\n",
        "print(decode(encode(\"Yashraj Sharma\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYuWR7-1hNiH",
        "outputId": "a52d2021-a4d9-4a41-ae33-73c9d09aefd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset dtype: torch.int64\n",
            "Dataset shape: torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "####converted all of Shakespeare into a single 1-D tensor of integers.\n",
        "text_dataset=torch.tensor(encode(text),dtype=torch.long)\n",
        "print(f\"Dataset dtype: {text_dataset.dtype}\")\n",
        "print(f\"Dataset shape: {text_dataset.shape}\")\n",
        "print(text_dataset[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVZpSm8jiBJP"
      },
      "source": [
        "##Dataset split for training and validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-06OgUNiF0U",
        "outputId": "b31e0ecd-df96-42dd-ec70-229815e698e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lenght of train dataset: 1003854\n",
            "Lenght of test dataset: 111540\n"
          ]
        }
      ],
      "source": [
        "#data_size of split:\n",
        "data_size=int(0.9*len(text_dataset))\n",
        "train_data=text_dataset[:data_size]\n",
        "test_data=text_dataset[data_size:]\n",
        "print(f\"Lenght of train dataset: {len(train_data)}\")\n",
        "print(f\"Lenght of test dataset: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSMbmIAdj3nJ",
        "outputId": "e1aee8e8-5133-4d05-df9b-09bd264e59b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context characters: tensor([18]), Target Character: 47\n",
            "Context characters: tensor([18, 47]), Target Character: 56\n",
            "Context characters: tensor([18, 47, 56]), Target Character: 57\n",
            "Context characters: tensor([18, 47, 56, 57]), Target Character: 58\n",
            "Context characters: tensor([18, 47, 56, 57, 58]), Target Character: 1\n",
            "Context characters: tensor([18, 47, 56, 57, 58,  1]), Target Character: 15\n",
            "Context characters: tensor([18, 47, 56, 57, 58,  1, 15]), Target Character: 47\n",
            "Context characters: tensor([18, 47, 56, 57, 58,  1, 15, 47]), Target Character: 58\n"
          ]
        }
      ],
      "source": [
        "#instead of training on whole chunck of dataset, we will train on random chucks of data:\n",
        "#Basically the earlier tokens will be the context character for next target token:\n",
        "block_size=8 #The model will never look at more than 8 past tokens\n",
        "x=train_data[:block_size] #x[t] is the context\n",
        "y=train_data[1:block_size+1] #y[t] is the correct next token\n",
        "for t in range(block_size):\n",
        "  context=x[:t+1]\n",
        "  target=y[t]\n",
        "  print(f\"Context characters: {context}, Target Character: {target}\")\n",
        "#transformer will everytime get the chunk size of same to predict next character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D9g8c7gql49L",
        "outputId": "f002bead-f523-4635-c064-88ff8ee00004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "target:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----------------------------------------------------\n",
            "Inputs: [24], Target: 43\n",
            "Inputs: [24, 43], Target: 58\n",
            "Inputs: [24, 43, 58], Target: 5\n",
            "Inputs: [24, 43, 58, 5], Target: 57\n",
            "Inputs: [24, 43, 58, 5, 57], Target: 1\n",
            "Inputs: [24, 43, 58, 5, 57, 1], Target: 46\n",
            "Inputs: [24, 43, 58, 5, 57, 1, 46], Target: 43\n",
            "Inputs: [24, 43, 58, 5, 57, 1, 46, 43], Target: 39\n",
            "Inputs: [44], Target: 53\n",
            "Inputs: [44, 53], Target: 56\n",
            "Inputs: [44, 53, 56], Target: 1\n",
            "Inputs: [44, 53, 56, 1], Target: 58\n",
            "Inputs: [44, 53, 56, 1, 58], Target: 46\n",
            "Inputs: [44, 53, 56, 1, 58, 46], Target: 39\n",
            "Inputs: [44, 53, 56, 1, 58, 46, 39], Target: 58\n",
            "Inputs: [44, 53, 56, 1, 58, 46, 39, 58], Target: 1\n",
            "Inputs: [52], Target: 58\n",
            "Inputs: [52, 58], Target: 1\n",
            "Inputs: [52, 58, 1], Target: 58\n",
            "Inputs: [52, 58, 1, 58], Target: 46\n",
            "Inputs: [52, 58, 1, 58, 46], Target: 39\n",
            "Inputs: [52, 58, 1, 58, 46, 39], Target: 58\n",
            "Inputs: [52, 58, 1, 58, 46, 39, 58], Target: 1\n",
            "Inputs: [52, 58, 1, 58, 46, 39, 58, 1], Target: 46\n",
            "Inputs: [25], Target: 17\n",
            "Inputs: [25, 17], Target: 27\n",
            "Inputs: [25, 17, 27], Target: 10\n",
            "Inputs: [25, 17, 27, 10], Target: 0\n",
            "Inputs: [25, 17, 27, 10, 0], Target: 21\n",
            "Inputs: [25, 17, 27, 10, 0, 21], Target: 1\n",
            "Inputs: [25, 17, 27, 10, 0, 21, 1], Target: 54\n",
            "Inputs: [25, 17, 27, 10, 0, 21, 1, 54], Target: 39\n"
          ]
        }
      ],
      "source": [
        "#Generalizing for parallel batch processing:\n",
        "torch.manual_seed(1337)\n",
        "batch_size=4     #How many batches of data is parallely trained.\n",
        "block_size=8     #What is the size of the data Chunk.\n",
        "\n",
        "def batch_creation(split):\n",
        "  #This function Generates small batches of data with inputs x and targets y\n",
        "  data=train_data if split=='train' else test_data\n",
        "  ix=torch.randint(len(data)-block_size,(batch_size,)) #randomly selecting the starting point of input text of shape(batch_size)\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix]) #input Sequence (4*8) dimension batch size of 32 input characters\n",
        "  y=torch.stack([data[i+1:i+block_size+1] for i in ix]) #output sequence  (4*8) dimension batch size of 32 output characters\n",
        "  return x,y\n",
        "\n",
        "\n",
        "input,tar=batch_creation('train')\n",
        "print(\"inputs:\")\n",
        "print(input.shape)\n",
        "print(input)\n",
        "print(\"target:\")\n",
        "print(tar.shape)\n",
        "print(tar)\n",
        "\n",
        "\n",
        "print(\"----------------------------------------------------\")\n",
        "\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context=input[b,:t+1]\n",
        "    target=tar[b,t] #The current token at position t is allowed to see all tokens before it (and itself), but nothing after it.\n",
        "    print(f\"Inputs: {context.tolist()}, Target: {target}\")\n",
        "\n",
        "#What makes GPT autoregressive?\n",
        "#Causal self-attention ensures each token only sees past tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVkEmA11seqf"
      },
      "source": [
        "#Bi-gram Language Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ovlBMZ-sVNq",
        "outputId": "97b03143-323e-4859-94c2-89577698f65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "#Bigram-Because the prediction for the next token depends only on the current token.\n",
        "#Bigram:\n",
        "#current token -> next-token distribution\n",
        "#Predict the next token using everything generated so far.”\n",
        "#######This is the entire point of attention.########\n",
        "class BigramModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    #each token directly reads off the logits for the next token from a lookup table...\n",
        "    self.Token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,idx,target=None):\n",
        "    logits=self.Token_embedding_table(idx) #you don’t just get a single number — you get the entire row of the embedding matrix that corresponds to that token ID.(Batch,Time,channel)\n",
        "    if target==None:\n",
        "      loss=0\n",
        "    else: # we are basically predicting the next character based on identity of a single token...\n",
        "        # loss=nn.CrossEntropyLoss(logits,target) (cannot work becuase(B,T,C)) cross entropy expect.(N,C)\n",
        "        B,T,C=logits.shape # unpacking...\n",
        "        logits_flat=logits.view(B*T,C) #2 dim instead of 3 dim(B,T,C) because Pytorch want input in this shape...\n",
        "        target_flat=target.view(B*T)\n",
        "        loss_fn=nn.CrossEntropyLoss() #no need to apply softmax for probabilty conversion as CrossEntropyloss=logsoftmax+negative log likelihood loss\n",
        "        loss=loss_fn(logits_flat,target_flat)\n",
        "\n",
        "    return logits,loss #(B,T,C) logit(4,8,65)[input shape,,vocab]...\n",
        "\n",
        "\n",
        "  def generate(self,idx,max_token):\n",
        "    #idx is (B,T) array of indices in the current context...\n",
        "    for _ in range(max_token):\n",
        "      #get the prediction:\n",
        "      logits,loss=self(idx)\n",
        "      #focusing only on the last step...\n",
        "      logits=logits[:,-1,:]#(we only want the Batch_size and channel size)(B,C)\n",
        "      #Softmax for probabilities....\n",
        "      probi=nn.Softmax(dim=1)\n",
        "      prob=probi(logits)\n",
        "      #Sample from distribution...\n",
        "      idx_next=torch.multinomial(prob,num_samples=1) #finding the next index element to be generated...(B,1)\n",
        "      idx=torch.cat((idx,idx_next),dim=1)#(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "bigram_model=BigramModel(vocab_size)\n",
        "output_ar,loss=bigram_model(input,tar)\n",
        "print(output_ar.shape)\n",
        "print(loss)\n",
        "print(decode(bigram_model.generate(torch.zeros((1,1),dtype=torch.long),max_token=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw7oZugFwWS7"
      },
      "source": [
        "## Model Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o4ykYA2wV0S"
      },
      "outputs": [],
      "source": [
        "#Loss Function and optimizer:\n",
        "optimizer=torch.optim.AdamW(bigram_model.parameters(),lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWNrRnSCxC12",
        "outputId": "7159c88c-e446-464a-9c0c-9e9e98fa65bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5319576263427734\n"
          ]
        }
      ],
      "source": [
        "batch_size=32\n",
        "\n",
        "for step in range(100000):\n",
        "  #sample form the dataset:\n",
        "  xb,yb=batch_creation('train')\n",
        "\n",
        "  #Pytorch Loop:\n",
        "  logits,loss=bigram_model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwkY5mOz_-Yh",
        "outputId": "17e4f79b-e4a7-4312-fbd5-7d030ccc5b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "KI, ghyone'lisurerat ms The Bulin the theid IAm VOMal fis,\n",
            "OMarethay:\n",
            "\n",
            "MESheelore f bu hasen, t 'Wharong t bu s yove tend n, I:\n",
            "LO:\n",
            "cthy cotscu\n",
            "ICHEdee r chidots\n",
            "Whe methare f mave.\n",
            "TEYo.\n",
            "Fo ou, t h Exf it se ventifitou, osingave brd Gilinousthathele w ak ad g torulor wrail Je t ts osh se t ay howoutreputeeed siveica he qureades't insecoof wheagheateril th anonswa tt ald d n sm; she CEr you s n ENES: s y wird\n",
            "Whit; alothers ws t y cthn;\n",
            "Whasery se h ofe fise pest, oun Mou thar tlul be fearsthee \n"
          ]
        }
      ],
      "source": [
        "print(decode(bigram_model.generate(torch.zeros((1,1),dtype=torch.long),max_token=500)[0].tolist()))#Max_token means the maximum number of token the model will generate..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-736qCBBCaY"
      },
      "source": [
        "### So Until Now in this model the token was only talking to the previous token now we want to store the pattern and sequence meaning of the whole sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqd16B6Xy7B"
      },
      "source": [
        "# The Mathematical Trick in Self-Attention:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5vqLzXdBCJZ",
        "outputId": "0ea38a78-a929-4dcc-b976-974f9227f603"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C=4,8,2\n",
        "x=torch.randn(B,T,C) #batch,Token,Channel\n",
        "x.shape\n",
        "#Simplest way to establish Communcation between Tokens is to average out the past Channels in order to find the cuurent token..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-hK05HtgUc6"
      },
      "outputs": [],
      "source": [
        "xbow=torch.zeros((B,T,C))\n",
        "for b in range(B): #Basically Taking Average of the current and past tokens at each step...\n",
        "  for t in range(T):\n",
        "    xprev=x[b,:t+1]\n",
        "    xbow[b,t]=torch.mean(xprev,0)\n",
        "#This way is very Inefficient...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KVsNwACmqeO",
        "outputId": "40187509-f11c-4fa8-f34f-dca345005391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ],
      "source": [
        "# version 2:\n",
        "wei=torch.tril(torch.ones(T,T))\n",
        "wei=wei/wei.sum(1,keepdim=True)\n",
        "xbow2=wei@x\n",
        "print(x[0])\n",
        "print(xbow2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Ly1B2joCc9",
        "outputId": "41141f38-3e91-4f66-aeaf-664b565845b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "T = 8  #version 3\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = torch.softmax(wei, dim=-1)\n",
        "\n",
        "wei\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Po9XHlhRzZ",
        "outputId": "118a5dec-7efb-44bc-d8fd-40c5f3a8ec94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "#version 1:\n",
        "### So we will use Matrix Multiplication:\n",
        "torch.manual_seed(42)\n",
        "a=torch.tril(torch.ones(3,3))\n",
        "#tensor([[1., 0., 0.],\n",
        "        #[1., 1., 0.],\n",
        "        #[1., 1., 1.]])\n",
        "#Interpretation:\n",
        "#Row 0 → can see token 0\n",
        "#Row 1 → can see tokens 0,1\n",
        "#Row 2 → can see tokens 0,1,2\n",
        "#This is a causal mask.\n",
        "\n",
        "#output=softmax(mask)×values\n",
        "#This is self-attention without learning.\n",
        "#In the first row, only token 0 is visible, so after normalization it receives the full weight of 1.0.\n",
        "\n",
        "#Row-wise normalization makes all visible tokens contribute equally\n",
        "\n",
        "print(a)\n",
        "print(\"--\")\n",
        "a=a/torch.sum(a,1,keepdim=True)#Normalization\n",
        "#normalized weights × value vectors (matrix multiplication)\n",
        "b=torch.randint(0,10,(3,2)).float()\n",
        "c=a@b\n",
        "print(a)\n",
        "print('--')\n",
        "print(b)\n",
        "print('--')\n",
        "print(c)\n",
        "#Self-attention works by deciding how much each visible past token contributes to forming the context used for the next-token prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLmcUiMpzfTA"
      },
      "source": [
        "past tokens\n",
        "   ↓\n",
        "embeddings\n",
        "   ↓\n",
        "attention (context distribution over past tokens)\n",
        "   ↓\n",
        "context vector\n",
        "   ↓\n",
        "linear head\n",
        "   ↓\n",
        "vocabulary distribution (next-token probabilities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR05YIGlzbVl"
      },
      "source": [
        "Attention decides which past tokens matter\n",
        "\n",
        "Those tokens shape the context vector\n",
        "\n",
        "The context vector determines which vocabulary tokens get high logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiGHXF7c1abG"
      },
      "source": [
        "# Here is the precise, correct flow of a GPT-style language model:\n",
        "\n",
        "Input tokens (all past tokens, including the current one)\n",
        "Token embedding\n",
        "encodes what the token is\n",
        "Positional embedding\n",
        "encodes where the token is\n",
        "\n",
        "Add token + position embeddings\n",
        "\n",
        "gives an ordered sequence of vectors\n",
        "\n",
        "Self-attention mechanism\n",
        "\n",
        "computes how much each past token contributes\n",
        "\n",
        "produces context vectors\n",
        "\n",
        "Linear (LM) head\n",
        "\n",
        "maps context vectors to vocabulary logits\n",
        "\n",
        "Softmax\n",
        "\n",
        "converts logits into a probability distribution\n",
        "\n",
        "Next-token prediction\n",
        "\n",
        "That is exactly the full pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vVTyKUsC7uC"
      },
      "source": [
        "# Each token in every single position will contain two vectors -> query:What am I looking for? Key:What do I contain? And to get affinity between these two be do the Dot Product of both vector\n",
        "So my query dot product all the keys of the tokens and The dot products become scores, which are then normalized via softmax to become attention weights...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TJhVU4MRzawN"
      },
      "outputs": [],
      "source": [
        "#Implementing Attention Mechanism:\n",
        "torch.manual_seed(1337)\n",
        "B,T,C=4,8,32\n",
        "x=torch.randn(B,T,C)\n",
        "\n",
        "#Single head self attention:\n",
        "head_size=16\n",
        "key=nn.Linear(C,head_size,bias=False)\n",
        "query=nn.Linear(C,head_size,bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k=key(x) # (B,T,16)\n",
        "q=query(x) # (B,T,16)\n",
        "v=value(x) # X is private to this token...V carries the actual information that flows forward...\n",
        "wei=q@k.transpose(-2,-1)*(head_size**-0.5) #(B,T,16) @ (B,16,T) --> (B,T,T)\n",
        "#Scaling is used to control the variance of the dot-product scores at initialization.\n",
        "tril=torch.tril(torch.ones(T,T))\n",
        "wei=wei.masked_fill(tril==0,float('-inf'))\n",
        "wei=torch.softmax(wei,dim=-1)\n",
        "out=wei@v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQtgNv2fGqj-",
        "outputId": "93e4d62f-8c90-4a85-ab6b-453f4a7294cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
              "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
              "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGmreq7xKpl0"
      },
      "source": [
        "### The batch dimension is purely for parallel computation; each batch element is an independent sequence and never exchanges information with other batch elements.\n",
        "# Each token can interact only with other tokens within the same batch element (i.e., within the same sequence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdWCeTXkMBKG"
      },
      "source": [
        "### Encoder self-attention can see all tokens (past + future), while decoder self-attention can see only past and current tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conceptually, MultiHead Attention with n_embd=32:\n",
        "\n",
        "Head 0: operates on 8 channels\n",
        "Head 1: operates on 8 channels\n",
        "Head 2: operates on 8 channels\n",
        "Head 3: operates on 8 channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
